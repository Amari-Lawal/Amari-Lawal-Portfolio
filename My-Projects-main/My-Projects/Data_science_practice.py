# -*- coding: utf-8 -*-
"""Data Science Practice

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g6tWThIVePAta0saH5iA5cfBmmlRqWj5

---

# Import modules

# Logistic Regression with age and Trestbps

*Use Kaggle to understand the columns.


*  Trestbps = presting blood pressure (in mm Hg on admission to the hospital)
*  cp = chest pain type
*   chol = serum cholestoral in mg/dl
*

Heart Conditions Project

Context
This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to
this date. The "goal" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4.

Content

Attribute Information:

age
sex
chest pain type (4 values)
resting blood pressure
serum cholestoral in mg/dl
fasting blood sugar > 120 mg/dl
resting electrocardiographic results (values 0,1,2)
maximum heart rate achieved
exercise induced angina
oldpeak = ST depression induced by exercise relative to rest
the slope of the peak exercise ST segment
number of major vessels (0-3) colored by flourosopy
thal: 3 = normal; 6 = fixed defect; 7 = reversable defect
The names and social security numbers of the patients were recently removed from the database, replaced with dummy values. One file has been "processed", that one containing the Cleveland database. All four unprocessed files also exist in this directory.

To see Test Costs (donated by Peter Turney), please see the folder "Costs"

Acknowledgements
Creators:

Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.
University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.
University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.
V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.
Donor:
David W. Aha (aha '@' ics.uci.edu) (714) 856-8779

Inspiration
Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).

See if you can find any other trends in heart data to predict certain cardiovascular events or find any clear indications of heart health.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn			
import sklearn.datasets	
import sklearn.linear_model		 
import sklearn.metrics
import random
import tensorflow as tf
from sklearn import tree
import random
import seaborn as sns

#https://www.kaggle.com/ronitf/heart-disease-uci

# Load data 
DATA_URL = "/content/heart.csv"

df = pd.read_csv(DATA_URL,one_hot=True)
display(df)
df_n = df.to_numpy()
display(df)

# This is was in my early time of machine learning
X, y = df_n[:,:], df_n[:,:] # Not really effective in any way
print(df_n.shape)
plt.scatter(X,y)
plt.xlabel("Age")
plt.ylabel("trestbps")
plt.show()

#x = df.loc[:,"age"]
#y = df.loc[:,"chol"]
#plt.scatter(x,y)
#plt.xlabel("Age")
#plt.ylabel("serum cholestoral in mg/dl")
#plt.show()

def train_val_test_split(dataset):
  return np.split(dataset, [int(dataset.shape[0] * 0.8), int(dataset.shape[0] * 0.9)]) # Splits data

X_train,X_val,X_test = train_val_test_split(X)
y_train,y_val,y_test = train_val_test_split(y)

X_train = X_train.reshape(242,1)
y_train = y_train.reshape(242,1)

model = sklearn.linear_model.LinearRegression() # Creates model doesn't really tell us anything seeing theres no labels in a Supervised learning method
model.fit(X_train,y_train)
w = model.coef_
b = model.intercept_
print(w)

yhat_train = model.predict(X_train)
yhat_val = model.predict(X_val)
print(sklearn.metrics.mean_squared_error(yhat_train,y_train))
print(sklearn.metrics.mean_squared_error(yhat_val,y_val)) # evaluate

"""# **Attempt 2**"""

df = pd.DataFrame(df)
display(df)

df_age = df.iloc[:,0]
df_trestbps = df.iloc[:,3] # I realise labels are required
X, y = df_age,df_trestbps
X = X.to_numpy()
y = y.to_numpy()
X.reshape(303,1)
y.reshape(303,1)

X2 = np.log(df_age)
y2 = np.log(df_trestbps)
X2 = X2.to_numpy()
y2 = y2.to_numpy()

plt.scatter(X,y)
plt.xlabel("Age")
plt.ylabel("trestbps")
plt.show()

def train_val_test_split(dataset):
  return np.split(dataset, [int(dataset.shape[0] * 0.8), int(dataset.shape[0] * 0.9)])

X_train,X_val,X_test = train_val_test_split(X2)
y_train,y_val,y_test = train_val_test_split(y2)

X_train = X_train.reshape(121,2)
y_train = y_train.reshape(121,2)
X_val = X_val.reshape(15,2)
y_val = y_val.reshape(15,2)

model_2 = sklearn.linear_model.LinearRegression()
model_2.fit(X_train,y_train)
w = model_2.coef_
b = model_2.intercept_
print(w)

# Lookin for linear relationship
yhat_train = model_2.predict(X_train)
yhat_val = model_2.predict(X_val)
print(sklearn.metrics.mean_squared_error(yhat_train,y_train))
print(sklearn.metrics.mean_squared_error(yhat_val,y_val))
#Closer to zero is a good MSE

r = y_val - yhat_val 
print(r) # Found residual which was good with a random distribution of data
plt.scatter(r,yhat_val)

"""# Titanic Machine Learning

ðŸ‘‹ðŸ›³ï¸ Ahoy, welcome to Kaggle! Youâ€™re in the right place.
This is the legendary Titanic ML competition â€“ the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.

The competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.

Read on or watch the video below to explore more details. Once youâ€™re ready to start competing, click on the "Join Competition button to create an account and gain access to the competition data. Then check out Alexis Cookâ€™s Titanic Tutorial that walks you through step by step how to make your first submission!



The Challenge
The sinking of the Titanic is one of the most infamous shipwrecks in history.

On April 15, 1912, during her maiden voyage, the widely considered â€œunsinkableâ€ RMS Titanic sank after colliding with an iceberg. Unfortunately, there werenâ€™t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.

While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.

In this challenge, we ask you to build a predictive model that answers the question: â€œwhat sorts of people were more likely to survive?â€ using passenger data (ie name, age, gender, socio-economic class, etc).

Recommended Tutorial
We highly recommend Alexis Cookâ€™s Titanic Tutorial that walks you through making your very first submission step by step.
Overview of How Kaggleâ€™s Competitions Work
1. Join the Competition

Read about the challenge description, accept the Competition Rules and gain access to the competition dataset.

2. Get to Work

Download the data, build models on it locally or on Kaggle Kernels (our no-setup, customizable Jupyter Notebooks environment with free GPUs) and generate a prediction file.

3. Make a Submission

Upload your prediction as a submission on Kaggle and receive an accuracy score.

4. Check the Leaderboard

See how your model ranks against other Kagglers on our leaderboard.

5. Improve Your Score

Check out the discussion forum to find lots of tutorials and insights from other competitors.

Kaggle Lingo Video
You may run into unfamiliar lingo as you dig into the Kaggle discussion forums and public notebooks. Check out Dr. Rachael Tatmanâ€™s video on Kaggle Lingo to get up to speed!
What Data Will I Use in This Competition?
In this competition, youâ€™ll gain access to two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.

Train.csv will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the â€œground truthâ€.

The `test.csv` dataset contains similar information but does not disclose the â€œground truthâ€ for each passenger. Itâ€™s your job to predict these outcomes.

Using the patterns you find in the train.csv data, predict whether the other 418 passengers on board (found in test.csv) survived.

Check out the â€œDataâ€ tab to explore the datasets even further. Once you feel youâ€™ve created a competitive model, submit it to Kaggle to see where your model stands on our leaderboard against other Kagglers.

How to Submit your Prediction to Kaggle
Once youâ€™re ready to make a submission and get on the leaderboard:

1. Click on the â€œSubmit Predictionsâ€ button



2. Upload a CSV file in the submission file format. Youâ€™re able to submit 10 submissions a day.



Submission File Format:

You should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.

The file should have exactly 2 columns:

PassengerId (sorted in any order)
Survived (contains your binary predictions: 1 for survived, 0 for deceased)
Got it! Iâ€™m ready to get started. Where do I get help if I need it?
For Competition Help: Titanic Discussion Forum
Technical Help: Kaggle Contact Us Page

Kaggle doesnâ€™t have a dedicated support team so youâ€™ll typically find that you receive a response more quickly by asking your question in the appropriate forum. The forums are full of useful information on the data, metric, and different approaches. We encourage you to use the forums often. If you share your knowledge, you'll find that others will share a lot in turn!

A Last Word on Kaggle Notebooks
As we mentioned before, Kaggle Notebooks is our no-setup, customizable, Jupyter Notebooks environment with free GPUs and a huge repository of community published data & code.

In every competition, youâ€™ll find many Kernels publically shared with incredible insights. Itâ€™s an invaluable resource worth becoming familiar with. Check out this competitionâ€™s Kernelâ€™s here.
"""

#https://www.kaggle.com/c/titanic

DATA_URL_TR = "/content/train.csv"

df_tr = pd.read_csv(DATA_URL_TR)
display(df_tr)

DATA_URL_GS = "/content/gender_submission.csv"

df_gs = pd.read_csv(DATA_URL_GS)
display(df_gs)
print(pd.unique(df_gs.iloc[:,0]))

DATA_URL_TE = "/content/test.csv"

df_te = pd.read_csv(DATA_URL_TE)
display(df_te)
X_test = df_te.loc[:,"Pclass"]
X_test = X_test.to_numpy()
X_test = X_test.reshape(418,1)

def column_bool_series(dataframe,column,bool_value): # Create function for finding basic boolean series to determine who died or lived
   overall_1st = dataframe.loc[:,column] == bool_value 
   overall_1st = overall_1st.sum()
   return overall_1st

column_bool_series(df_tr,"Survived",1)

# Check how many people died
#Check who died from id and survived

died_sum  = column_bool_series(df_tr,"Survived",0)
live_sum =  column_bool_series(df_tr,"Survived",1)
print("Died",died_sum)
print("Lived",live_sum)
perc_died = (died_sum / (live_sum+died_sum))* 100
perc_lived =  (live_sum/(live_sum+died_sum)* 100)
print("Percentage of who died",perc_died)
print("Percentage of who lived",perc_lived)

# Compare those who died to a threshold of how much the fair is.
# Compare the cheapest with those who died
# $ Compare Pclass with fair
#/ Heatmap x = passengerId, y = Pclass, z = Fare
print(pd.unique(df_tr["Pclass"]))
x = df_tr.loc[:,"Pclass"]
y = df_tr.loc[:,"Fare"]
z = df_tr.loc[:,"PassengerId"]
plt.scatter(x,y)
plt.xlabel("Pclass")
plt.ylabel("Fare")
plt.show()

# $ Person who bought the fare for 500 hundred did he live
# $ Find threshold of those who paid 200 or more and see if they lived, if so why?
fare_live_500 = df_tr.loc[:,"Fare"] > 500 
mask = fare_live_500
cols = ["Name","PassengerId","Fare","Pclass","Survived","SibSp"]
filt_df = df_tr.loc[mask,cols]
display(filt_df)
print("The Three richest people who bought the highest fair all survived")

df_unk = filt_df.iloc[:,]

# The Sum of People who paid a fare of 200 or more and who died
fare_over_200 = df_tr.loc[:,"Fare"] > 500
mask_over_200 = fare_over_200
cols = ["Name","PassengerId","Fare","Pclass","Survived","Age"]
filt_over_200_df = df_tr.loc[mask_over_200,cols]
display(filt_over_200_df.sort_values("Survived",ascending=True))
filt_200_surv = column_bool_series(filt_over_200_df,"Survived",1)
filt_200_dead = column_bool_series(filt_over_200_df,"Survived",0)
print("People who survived:",filt_200_surv.sum())
print("People who Died:",filt_200_dead.sum())
print("Percentage of people dead:",(filt_200_dead.sum()/filt_200_surv.sum()*100))
print("Percentage of who lived:", 100 - (filt_200_dead.sum()/filt_200_surv.sum()*100))

# Of those who died make a data frame of the potential variables
died = df_tr.loc[:,"Survived"] == 0
und_100 = df_tr.loc[:,"Fare"] < 500
mask_died_und = died & und_100
cols = ["PassengerId","Pclass","Survived","Fare"]

f_df = df_tr.loc[mask_died_und,cols]
f_df = f_df.rename(columns={"Survived":"Died"})
display(f_df)

# Trying to find how many people of one class died
print("Overall people on board:")

overall_1st = df_tr.loc[:,"Pclass"] == 1
overall_2nd = df_tr.loc[:,"Pclass"] == 2
overall_3rd = df_tr.loc[:,"Pclass"] == 3

print("1st class total on board:",overall_1st.sum())
print("2nd class total on board:",overall_2nd.sum())
print("3rd class total on board:",overall_3rd.sum())
print("Overall on board",216+184+491)

died_1st = f_df.loc[:,"Pclass"] == 1
died_2nd = f_df.loc[:,"Pclass"] == 2
died_3rd = f_df.loc[:,"Pclass"] == 3
print("People who died:")
print("1st class who died:",died_1st.sum())
print("2nd class who died:",died_2nd.sum())
print("3rd class who died:",died_3rd.sum())
print("Overall Dead",66+97+372)

print("Percentage of who died in each class:")
print("Percentage of 1st class who died",(died_1st.sum()/overall_1st.sum())*100)
print("Percentage of 2nd class who died",(died_2nd.sum()/overall_2nd.sum())*100)
print("Percentage of 3rd class who died",(died_3rd.sum()/overall_3rd.sum())*100)

display(df_tr)

# Trying to see if the amount of siblings someone has effects mortality rate.
print(pd.unique(df_tr.loc[:,"SibSp"]))
print((df_tr.loc[:,"SibSp"] == 1).sum())
mask_3 = df_tr.loc[:,"SibSp"] == 1 
cols_2 = ["PassengerId","Survived","Pclass","Name","Age","Sex","Embarked","SibSp"]
filt = df_tr.loc[mask_3,cols_2]
display(filt)

filt_died = column_bool_series(filt,"Survived",0)
filt_lived = column_bool_series(filt,"Survived",1)
total = filt_died + filt_lived
print("Percentage of who died",(filt_died/total) * 100)
print("Percentage of who lived",(100-(filt_died/total) * 100))

# Tring to find common relatives
df_tr_s = pd.Series(df_tr.loc[:,"Name"])
cols = ["Name","Age","SibSp","Pclass"]
mask = df_tr_s.str.contains(pat='Harris') 
print("Names with Harris in them:",mask.sum())

filt = df_tr.loc[mask,cols]

display(filt)




"""
mask = df_tr.loc[:,"Name"] == 
cols = ["Name","Age","SibSp"]
filt_ddf = df_tr.loc[mask,cols]
display(filt_ddf)
"""

#fare_bel_500 = df_tr.loc[:,"Fare"] < 500
fare_bel_500 = df_tr.loc[:,"Pclass"]
cols = ["Pclass"]
filt_bel_500 = df_tr.loc[fare_bel_500,cols]
filt_bel_500 = filt_bel_500.to_numpy()

#cols_500 = ["PassengerId","Fare","Pclass","Survived"]
#filt_over_500_df = df_tr.loc[fare_bel_500,cols_500]
#display(filt_over_500_df)
#print(filt_over_500_df.isnull().sum())

#Create dataset to feed into model with x = Pclass and y = Survived
for rand in range(0,3):
  randnum = random.randint(0,892)
#  print(randnum)
X_train = filt_bel_500
y_train = df_tr.loc[:,"Survived"]
y_train = pd.DataFrame(y_train)

#y_train = y_train.drop(randnum,axis=0)
#y_train = y_train.drop(208,axis=0)
#y_train = y_train.drop(258,axis=0)
print(y_train.shape)
X_train.shape

display(pd.DataFrame(X_train))
display(pd.DataFrame(y_train))

from sklearn import cluster
from sklearn import ensemble

#Get rid of outliers of the super rich people we identified before. 
model = sklearn.ensemble.RandomForestClassifier()
model.fit(X_train,y_train)

yhat_train = model.predict(X_train)
yhat_test = model.predict(X_test)

print(sklearn.metrics.accuracy_score(y_train,yhat_train))
print(sklearn.metrics.accuracy_score(X_test,yhat_test))

#Gender Submission
DATA_URL_GS = "/content/gender_submission.csv"

df_gs = pd.read_csv(DATA_URL_GS)
display(df_gs)

"""# Tensor Flow Practice

I'm tring to convert the Mandarin characters into unicode to format it properly and refrmat it into a numpy array to be fed into the linear model. May use Logistic regression or **Decision Tree Classifier**
"""

DATA_T_URL_DL = "/content/chinese_mnist.csv"
#DATA_T_URL = "https://raw.githubusercontent.com/Amari-Lawal/Data-Science-Practice/main/chinese_mnist.csv?token=AOQAAOESEC26FKK5RJEJYDS7SBDH2"
df = pd.read_csv(DATA_T_URL_DL) # Loads in dataset

display(df)

df_character = df.loc[:,"character"]
df_values = df.loc[:,"value"]
print(pd.unique(df_character).sum())
#print(df_values["ä¹"].sum())
print(pd.unique(df_values).sum())

#print((df_values == 1).sum())

#Not main trying to look at all unique values with the corresponding value
df_character = df.iloc[:,-1]
df_values = df.iloc[:,-2]

df_val_char = np.vstack((df_values,df_character))
#display(pd.DataFrame(pd.unique(df_val_char)))
df_val_char = pd.DataFrame(df_val_char)
print(pd.unique(df_val_char))

df_lables = df.iloc[:,3] # The number that is equivalent to the Mandarin character
df_train = df.iloc[:,-1] # indexes the labeles I can use to classify Mandarin characters 
df_train.shape # shows the shape of the Mandarin column 
df_lables_2 = df_lables.to_numpy()

print(df_lables_2)

uni_train = [] # creates a list to add the unicode of the character into 
uni_empty = np.zeros(15000) #Creates an "empty" numpy array full of zeros
uni_empty = uni_empty.reshape(15000,1) #Reshapes it to have a correct shape 

for chin_char in df_train: # Runs through each the Mandarin character
  char = ord(chin_char) # changes it into unicode
  uni_train.append(char) # Adds all the once Mandarin characters, now unicode ints into list
         # I'm tring to Add all the unicode characters in uni_trainn into a numpy array and reshape it to use in the model.
for i in range(len(uni_train)):
  uni_empty[i] = uni_train[i]
df_labels_2 = df_lables_2.reshape(15000,1)
uni_empty.reshape(15000,1)
print(uni_empty)
print(df_lables_2)
print(df_lables_2.shape)
print(uni_empty.shape)
#np.append(uni_empty,uni_train)

display(df_labels_2)
display(uni_empty)

def train_test_image(dataset):
  return np.split(dataset, [int(dataset.shape[0] * 0.8)]) # Fundtion to split the dataset into training set and test set

#Main
X_train_images, X_test_labels = train_test_image(uni_empty) # Uses function to split them and store them as variables.
y_train_images, y_test_labels = train_test_image(df_lables_2)

# Main Attempt 2 Using
X_train_images, X_test_images = train_test_image(uni_empty) 
y_train_labels, y_test_labels = train_test_image(df_lables_2)

print("X_train_images =", X_train_images.shape)
print("y_train_labels =",y_train_labels.shape)
print("X_test_images =",X_test_images.shape)
print("y_test_label =",y_test_labels.shape)

model = sklearn.tree.DecisionTreeClassifier(min_samples_leaf=2)
model.fit(X_train_images,y_train_labels)

model_knn = sklearn.linear_model.LogisticRegression(fit_intercept=True)
model_knn.fit(X_train_images,y_train_labels)

fig = plt.figure(figsize=(100,100))
_ = tree.plot_tree(model)

yhat_train = model.predict(X_train_images)
yhat_test = model.predict(X_test_images)

yhat_train_knn = model_knn.predict(X_train_images)
yhat_test_knn = model_knn.predict(X_test_images)

random_num = random.randint(0,1501)
letter_char = chr(X_test_labels[random_num])

print("Chinese value = ",letter_char)
print("Corresponding Denary value = ",yhat_test[random_num])

print(ord("a"))
print(chr(97))

print("Decision Tree Claasifier:")
print("Train set =",sklearn.metrics.f1_score(y_train_labels,yhat_train,average="macro"))
print("Test set = ",sklearn.metrics.f1_score(y_test_labels,yhat_test,average="macro"))

print("Logistic R:")
print("Train set =",sklearn.metrics.f1_score(y_train_labels,yhat_train_knn,average="macro")) # Didn't fully work
print("Test set = ",sklearn.metrics.f1_score(y_test_labels,yhat_test_knn,average="macro"))